{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567537cd",
   "metadata": {},
   "source": [
    "# Практическая работа 2: Кластеризация студентов\n",
    "\n",
    "## Анализ данных опроса с использованием методов машинного обучения\n",
    "\n",
    "**Выполнил:** Адаменко Семён Сергеевич, ИВТ 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80334967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr # Для коэффициента Фи\n",
    "\n",
    "# Установка библиотеки UMAP (если еще не установлена)\n",
    "# !pip install umap-learn\n",
    "import umap\n",
    "\n",
    "# Установка других библиотек для кластеризации\n",
    "# !pip install scikit-learn\n",
    "# !pip install fuzzy-c-means # Для Fuzzy C-Means, если будете использовать\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "# from fcmeans import FCM # Для Fuzzy C-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70db69",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 1. Понимание бизнес-задачи (Business Understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5e6a1",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 2. Понимание данных (Data Understanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51838a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Произошла ошибка при загрузке файла: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "try:\n",
    "    df = pd.read_excel('dlia studentov.xlsx')\n",
    "    print(\"Данные успешно загружены.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Ошибка: Файл 'dlia studentov.xlsx' не найден. Убедитесь, что он находится в той же директории.\")\n",
    "    df = pd.DataFrame() # Создаем пустой DataFrame, чтобы избежать ошибок далее\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при загрузке файла: {e}\")\n",
    "    df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed64ed2",
   "metadata": {},
   "source": [
    "### Первичный осмотр данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f0c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные не загружены. Пропустим первичный осмотр.\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"\\nРазмерность набора данных (строки, столбцы):\", df.shape)\n",
    "    print(\"\\nПервые 5 строк данных:\\n\", df.head())\n",
    "    print(\"\\nИнформация о типах данных:\\n\")\n",
    "    df.info()\n",
    "    print(\"\\nПропущенные значения в каждом столбце:\\n\", df.isnull().sum())\n",
    "else:\n",
    "    print(\"Данные не загружены. Пропустим первичный осмотр.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2853861",
   "metadata": {},
   "source": [
    "### Исследовательский анализ данных (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b074da",
   "metadata": {},
   "source": [
    "Для удобства работы, переименуем столбцы, чтобы они были короче и без специальных символов. Это облегчит доступ и читаемость кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554730d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные не загружены. Пропустим переименование колонок.\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    # Переименование колонок для удобства\n",
    "    new_column_names = {\n",
    "        'ID': 'ID',\n",
    "        'Время создания': 'Timestamp',\n",
    "        'На каком факультете/в каком институте Вы обучаетесь?': 'Faculty',\n",
    "        'Какая платформа для обучения дисциплине \"ИНФОКОММУНИКАЦИОННЫЕ ТЕХНОЛОГИИ\" использовалась?': 'Platform_Used',\n",
    "        'Был ли предусмотрен фидбек (отклик преподавателя на выполненное задание, например, указание ошибок и как их можно исправить)': 'Feedback_Provided',\n",
    "        'Необходим ли фидбек (отклик преподавателя на выполненное задание, например, указание ошибок и как их можно исправить) в электронном курсе?': 'Feedback_Needed',\n",
    "        'Был ли автоматический мониторинг присутствия студента на занятии (например, посредством QR-кодов)': 'Monitoring_Provided',\n",
    "        'Необходим ли автоматический мониторинг присутствия студента на занятии (например, посредством QR-кодов) в электронном курсе?': 'Monitoring_Needed',\n",
    "        'Материалы, представленные для практического задания, были в разных форматах (например, одновременно и текстовый, и видео)?': 'Materials_Varied_Formats',\n",
    "        'Необходимо ли представлять материалы для практического задания в разных форматах (например, одновременно и текстовый, и видео)?': 'Materials_Varied_Formats_Needed',\n",
    "        'Были ли для каждого Практического задания разработаны и опубликованы критерии оценивания?': 'Grading_Criteria_Provided',\n",
    "        'Необходимы ли для каждого Практического задания критерии оценивания?': 'Grading_Criteria_Needed',\n",
    "        'Был ли встроенный электронный журнал прогресса выполненных работ студентом?': 'Progress_Journal_Provided',\n",
    "        'Необходим ли встроенный электронный журнал прогресса выполненных работ студентом?': 'Progress_Journal_Needed',\n",
    "        'Были ли встроенны в электронный курс видеолекции?': 'Video_Lectures_Provided',\n",
    "        'Необходимо ли встраивать в электронный курс видеолекции?': 'Video_Lectures_Needed',\n",
    "        'Были ли встроенные в электронный курс тесты по материалом видео лекций?': 'Tests_Video_Lectures_Provided',\n",
    "        'Была ли предусмотрена рефлексия (отзыв) после выполнения каждого практического задания?': 'Reflection_Task_Provided',\n",
    "        'Необходима ли рефлексия (отзыв) после выполнения каждого практического задания?': 'Reflection_Task_Needed',\n",
    "        'Была ли предусмотрена рефлексия (отзыв) после завершения работы по дисциплине?': 'Reflection_Discipline_Provided',\n",
    "        'Необходима ли рефлексия (отзыв) после завершения работы по дисциплине?': 'Reflection_Discipline_Needed',\n",
    "        'Было ли организовано взаимодействие с преподавателями посредством мессенджеров?': 'Messenger_Interaction_Provided',\n",
    "        'Необходимо ли организовывать взаимодействие с преподавателями посредством мессенджеров?': 'Messenger_Interaction_Needed'\n",
    "    }\n",
    "    df.rename(columns=new_column_names, inplace=True)\n",
    "    print(\"\\nКолонки переименованы.\\n\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Данные не загружены. Пропустим переименование колонок.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938aa132",
   "metadata": {},
   "source": [
    "### Анализ отдельных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a31bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные не загружены. Пропустим EDA.\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    # Выбираем колонки, которые, по всей видимости, являются бинарными 'да'/'нет' и релевантны для кластеризации\n",
    "    # Исключаем 'Provided' (то, что было) и оставляем 'Needed' (то, что необходимо - т.е. предпочтения)\n",
    "    # Также исключаем 'Platform_Used', 'ID', 'Timestamp' и 'Faculty' на данном этапе.\n",
    "\n",
    "    binary_features_for_clustering = [\n",
    "        'Feedback_Needed',\n",
    "        'Monitoring_Needed',\n",
    "        'Materials_Varied_Formats_Needed',\n",
    "        'Grading_Criteria_Needed',\n",
    "        'Progress_Journal_Needed',\n",
    "        'Video_Lectures_Needed',\n",
    "        'Reflection_Task_Needed',\n",
    "        'Reflection_Discipline_Needed',\n",
    "        'Messenger_Interaction_Needed'\n",
    "    ]\n",
    "\n",
    "    print(\"\\nАнализ распределения ответов для бинарных признаков (предпочтения):\\n\")\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, col in enumerate(binary_features_for_clustering):\n",
    "        plt.subplot(3, 3, i + 1) # Размещаем графики в сетке 3x3\n",
    "        sns.countplot(x=col, data=df, palette='viridis')\n",
    "        plt.title(f'Распределение ответов: {col}')\n",
    "        plt.xlabel('') # Убираем подпись оси X для чистоты\n",
    "        plt.ylabel('Количество студентов')\n",
    "        # Добавляем частоты над столбцами\n",
    "        total = len(df[col])\n",
    "        for p in plt.gca().patches:\n",
    "            height = p.get_height()\n",
    "            plt.gca().text(p.get_x() + p.get_width()/2., height + 0.1,\n",
    "                           f'{height / total:.1%}', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Анализ распределения по факультетам\n",
    "    print(\"\\nРаспределение студентов по факультетам/институтам:\\n\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(y='Faculty', data=df, order=df['Faculty'].value_counts().index, palette='crest')\n",
    "    plt.title('Распределение студентов по факультетам/институтам')\n",
    "    plt.xlabel('Количество студентов')\n",
    "    plt.ylabel('Факультет/Институт')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Данные не загружены. Пропустим EDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b49a6",
   "metadata": {},
   "source": [
    "### Анализ связей между бинарными признаками (Коэффициент Фи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9cd29af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные не загружены. Пропустим расчет коэффициента Фи.\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    # Кодируем бинарные признаки для расчета коэффициента Фи ('да': 1, 'нет': 0, 'Moodle': 1, 'Авторская платформа': 0)\n",
    "    # Создаем копию для кодирования, чтобы не изменять исходный DataFrame df для EDA\n",
    "    df_encoded_phi = df[binary_features_for_clustering + ['Platform_Used']].copy()\n",
    "\n",
    "    # Применяем кодирование\n",
    "    for col in binary_features_for_clustering:\n",
    "        df_encoded_phi[col] = df_encoded_phi[col].map({'да': 1, 'нет': 0})\n",
    "\n",
    "    df_encoded_phi['Platform_Used'] = df_encoded_phi['Platform_Used'].map({'Moodle': 1, 'Авторская платформа': 0})\n",
    "\n",
    "    # Убедимся, что все значения числовые, где это возможно\n",
    "    df_encoded_phi = df_encoded_phi.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Удаляем строки с NaN, если они появились после кодирования (например, если были другие значения кроме 'да'/'нет')\n",
    "    df_encoded_phi.dropna(inplace=True)\n",
    "\n",
    "    # Расчет матрицы коэффициентов Фи\n",
    "    phi_matrix = pd.DataFrame(index=df_encoded_phi.columns, columns=df_encoded_phi.columns)\n",
    "\n",
    "    for col1 in df_encoded_phi.columns:\n",
    "        for col2 in df_encoded_phi.columns:\n",
    "            if col1 == col2:\n",
    "                phi_matrix.loc[col1, col2] = 1.0\n",
    "            else:\n",
    "                # Коэффициент Фи - это корреляция Пирсона для бинарных переменных\n",
    "                phi_matrix.loc[col1, col2] = pearsonr(df_encoded_phi[col1], df_encoded_phi[col2])[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(phi_matrix.astype(float), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "    plt.title('Матрица коэффициентов Фи для бинарных признаков')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Данные не загружены. Пропустим расчет коэффициента Фи.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065eba51",
   "metadata": {},
   "source": [
    "### Идентификация целевых признаков для кластеризации\n",
    "\n",
    "На основе анализа и понимания бизнес-задачи, для кластеризации будут использоваться **бинарные признаки, отражающие *потребности* и *предпочтения* студентов**. Признаки, касающиеся того, что *было* предусмотрено (`_Provided`), не будут использоваться напрямую для кластеризации, так как они описывают текущую ситуацию, а не желания студентов. Также не будут использоваться `ID`, `Timestamp` и `Faculty`.\n",
    "\n",
    "Список признаков для кластеризации:\n",
    "\n",
    "* `Feedback_Needed`\n",
    "* `Monitoring_Needed`\n",
    "* `Materials_Varied_Formats_Needed`\n",
    "* `Grading_Criteria_Needed`\n",
    "* `Progress_Journal_Needed`\n",
    "* `Video_Lectures_Needed`\n",
    "* `Reflection_Task_Needed`\n",
    "* `Reflection_Discipline_Needed`\n",
    "* `Messenger_Interaction_Needed`\n",
    "\n",
    "Признак `Faculty` (факультет/институт) будет сохранен для анализа распределения кластеров по факультетам на этапе оценки и визуализации, но не для самой кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe3a18",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 3. Подготовка данных (Data Preparation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41728612",
   "metadata": {},
   "source": [
    "### Очистка данных и кодирование категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f001c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные не загружены или не обработаны. Пропустим подготовку данных.\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    # Выбираем колонки для кластеризации (предпочтения)\n",
    "    features_for_clustering = [\n",
    "        'Feedback_Needed',\n",
    "        'Monitoring_Needed',\n",
    "        'Materials_Varied_Formats_Needed',\n",
    "        'Grading_Criteria_Needed',\n",
    "        'Progress_Journal_Needed',\n",
    "        'Video_Lectures_Needed',\n",
    "        'Tests_Video_Lectures_Provided', # Этот признак не был в 'Needed', но может быть релевантен\n",
    "        'Reflection_Task_Needed',\n",
    "        'Reflection_Discipline_Needed',\n",
    "        'Messenger_Interaction_Needed'\n",
    "    ]\n",
    "\n",
    "    # Добавляем 'Platform_Used' если нужно кодировать, хотя он не в 'Needed' группе\n",
    "    # df_processed = df[features_for_clustering + ['Faculty', 'Platform_Used']].copy()\n",
    "    df_processed = df[features_for_clustering + ['Faculty']].copy()\n",
    "\n",
    "    # Обработка пропущенных значений: Удаление строк с пропусками\n",
    "    # Обоснование: Для бинарных признаков, если ответ отсутствует, сложно адекватно его заменить.\n",
    "    # Удаление строк с пропусками является безопасным подходом, если их количество невелико.\n",
    "    initial_rows = df_processed.shape[0]\n",
    "    df_processed.dropna(inplace=True)\n",
    "    rows_after_dropna = df_processed.shape[0]\n",
    "    print(f\"Удалено строк с пропусками: {initial_rows - rows_after_dropna}\")\n",
    "\n",
    "    # Кодирование бинарных категориальных признаков в 0 и 1\n",
    "    # 'да' -> 1, 'нет' -> 0\n",
    "    # Здесь мы кодируем все 'Needed' признаки и, если бы Platform_Used был включен в features_for_clustering, его тоже\n",
    "    binary_mapping = {'да': 1, 'нет': 0}\n",
    "    for col in features_for_clustering:\n",
    "        df_processed[col] = df_processed[col].map(binary_mapping)\n",
    "\n",
    "    # Если 'Platform_Used' должен был быть закодирован как бинарный (Moodle: 1, Авторская платформа: 0)\n",
    "    # platform_mapping = {'Moodle': 1, 'Авторская платформа': 0}\n",
    "    # if 'Platform_Used' in df_processed.columns:\n",
    "    #    df_processed['Platform_Used'] = df_processed['Platform_Used'].map(platform_mapping)\n",
    "\n",
    "    # Проверяем типы данных после кодирования\n",
    "    print(\"\\nПервые 5 строк обработанных данных:\\n\", df_processed.head())\n",
    "    print(\"\\nТипы данных после кодирования:\\n\")\n",
    "    df_processed.info()\n",
    "\n",
    "    # Отделяем признаки для кластеризации от остальных\n",
    "    X_clustering = df_processed[features_for_clustering]\n",
    "    faculty_info = df_processed['Faculty'] # Информация о факультетах для последующей оценки\n",
    "\n",
    "    print(\"\\nРазмерность данных для кластеризации:\", X_clustering.shape)\n",
    "else:\n",
    "    print(\"Данные не загружены или не обработаны. Пропустим подготовку данных.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01bd29c",
   "metadata": {},
   "source": [
    "### Снижение размерности с использованием UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b41c52",
   "metadata": {},
   "source": [
    "UMAP - мощный алгоритм для снижения размерности, который хорошо сохраняет как локальную, так и глобальную структуру данных. Это особенно полезно для визуализации и улучшения качества кластеризации в высокоразмерных пространствах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e994791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные для кластеризации не подготовлены. Пропустим снижение размерности UMAP.\n"
     ]
    }
   ],
   "source": [
    "if 'X_clustering' in locals() and not X_clustering.empty:\n",
    "    # Подбор гиперпараметров UMAP (пример)\n",
    "    # n_neighbors: Чем меньше, тем больше UMAP фокусируется на локальной структуре. Чем больше, тем на глобальной.\n",
    "    # min_dist: Минимальное расстояние между точками во встроенном пространстве. Низкие значения - более плотные кластеры.\n",
    "    \n",
    "    n_neighbors_values = [5, 15, 30]\n",
    "    min_dist_values = [0.0, 0.1, 0.5]\n",
    "\n",
    "    umap_results = {}\n",
    "\n",
    "    print(\"\\nЭксперименты с UMAP:\\n\")\n",
    "    for n_neighbors in n_neighbors_values:\n",
    "        for min_dist in min_dist_values:\n",
    "            print(f\"  UMAP с n_neighbors={n_neighbors}, min_dist={min_dist}\")\n",
    "\n",
    "            # UMAP до 2-х компонент\n",
    "            reducer_2d = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2, random_state=42)\n",
    "            embedding_2d = reducer_2d.fit_transform(X_clustering)\n",
    "            umap_results[f'2D_n{n_neighbors}_d{min_dist}'] = embedding_2d\n",
    "\n",
    "            # UMAP до 3-х компонент\n",
    "            reducer_3d = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=3, random_state=42)\n",
    "            embedding_3d = reducer_3d.fit_transform(X_clustering)\n",
    "            umap_results[f'3D_n{n_neighbors}_d{min_dist}'] = embedding_3d\n",
    "\n",
    "            # Визуализация 2D-представления для оценки\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(embedding_2d[:, 0],\n",
    "                        embedding_2d[:, 1],\n",
    "                        s=5, alpha=0.8)\n",
    "            plt.title(f'UMAP 2D-представление (n_neighbors={n_neighbors}, min_dist={min_dist})')\n",
    "            plt.xlabel('UMAP Dimension 1')\n",
    "            plt.ylabel('UMAP Dimension 2')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    print(\"\\nВыводы по подбору гиперпараметров UMAP:\\n\")\n",
    "    print(\"   - Визуально оцените, какие комбинации `n_neighbors` и `min_dist` создают наиболее четкие и разделенные группы точек. Это будет субъективно, но поможет выбрать лучшие параметры для кластеризации.\")\n",
    "    print(\"   - Например, более низкий `min_dist` (ближе к 0) обычно приводит к более компактным кластерам, а `n_neighbors` регулирует баланс между локальной и глобальной структурой.\")\n",
    "\n",
    "    # Выбор оптимальных параметров UMAP (на основе визуальной оценки или более сложных метрик)\n",
    "    # Для примера выберем некие параметры для дальнейшей работы\n",
    "    best_n_neighbors = 15\n",
    "    best_min_dist = 0.1\n",
    "\n",
    "    # Окончательное UMAP-преобразование с выбранными параметрами\n",
    "    print(f\"\\nОкончательное UMAP-преобразование с выбранными параметрами: n_neighbors={best_n_neighbors}, min_dist={best_min_dist}\")\n",
    "    reducer_2d_final = umap.UMAP(n_neighbors=best_n_neighbors, min_dist=best_min_dist, n_components=2, random_state=42)\n",
    "    X_umap_2d = reducer_2d_final.fit_transform(X_clustering)\n",
    "\n",
    "    reducer_3d_final = umap.UMAP(n_neighbors=best_n_neighbors, min_dist=best_min_dist, n_components=3, random_state=42)\n",
    "    X_umap_3d = reducer_3d_final.fit_transform(X_clustering)\n",
    "\n",
    "    print(\"Созданы 2D и 3D UMAP-представления данных.\")\n",
    "else:\n",
    "    print(\"Данные для кластеризации не подготовлены. Пропустим снижение размерности UMAP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68792bc6",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 4. Моделирование (Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb7d6f",
   "metadata": {},
   "source": [
    "На этом этапе мы применим различные алгоритмы кластеризации к исходным (закодированным) данным, а также к 2D и 3D представлениям, полученным с помощью UMAP. Мы также проведем подбор гиперпараметров для каждого алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f38b1e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные для кластеризации не подготовлены. Пропустим этап моделирования.\n"
     ]
    }
   ],
   "source": [
    "if 'X_clustering' in locals() and not X_clustering.empty and 'X_umap_2d' in locals():\n",
    "    # Определим наборы данных для кластеризации\n",
    "    datasets_for_clustering = {\n",
    "        'Original_Binary': X_clustering,\n",
    "        'UMAP_2D': X_umap_2d,\n",
    "        'UMAP_3D': X_umap_3d\n",
    "    }\n",
    "\n",
    "    # Алгоритмы кластеризации для тестирования\n",
    "    clustering_algorithms = {\n",
    "        'KMeans': KMeans(random_state=42, n_init=10), # n_init=10 для избежания предупреждения\n",
    "        'AgglomerativeClustering': AgglomerativeClustering(),\n",
    "        'DBSCAN': DBSCAN(),\n",
    "        'GaussianMixture': GaussianMixture(random_state=42),\n",
    "        # 'FuzzyCMeans': FCM(n_clusters=3, random_state=42) # Если установлен fuzzy-c-means\n",
    "    }\n",
    "\n",
    "    # Словарь для хранения результатов кластеризации (лейблы кластеров)\n",
    "    cluster_labels = {}\n",
    "\n",
    "    for ds_name, data in datasets_for_clustering.items():\n",
    "        print(f\"\\n--- Кластеризация на наборе данных: {ds_name} ---\")\n",
    "\n",
    "        for algo_name, algo_model in clustering_algorithms.items():\n",
    "            print(f\"  Обучение {algo_name}...\")\n",
    "\n",
    "            if algo_name == 'KMeans' or algo_name == 'GaussianMixture':\n",
    "                # Подбор K для K-Means и GMM (метод локтя, силуэт)\n",
    "                # Поскольку это кластеризация без известного K, мы будем тестировать диапазоны\n",
    "                # и использовать метрики для оценки оптимального K\n",
    "                possible_k_values = range(2, 7) # Например, от 2 до 6 кластеров\n",
    "                inertia_values = []\n",
    "                silhouette_scores = []\n",
    "                db_scores = []\n",
    "\n",
    "                for k in possible_k_values:\n",
    "                    if algo_name == 'KMeans':\n",
    "                        model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                    else: # GaussianMixture\n",
    "                        model = GaussianMixture(n_components=k, random_state=42)\n",
    "\n",
    "                    model.fit(data)\n",
    "                    labels = model.predict(data) if hasattr(model, 'predict') else model.labels_\n",
    "\n",
    "                    if algo_name == 'KMeans':\n",
    "                        inertia_values.append(model.inertia_)\n",
    "                    \n",
    "                    # Метрики силуэта и Дэвиса-Болдина требуют как минимум 2 кластера\n",
    "                    if len(np.unique(labels)) > 1:\n",
    "                        silhouette_scores.append(silhouette_score(data, labels))\n",
    "                        db_scores.append(davies_bouldin_score(data, labels))\n",
    "                    else:\n",
    "                        silhouette_scores.append(np.nan)\n",
    "                        db_scores.append(np.nan)\n",
    "                \n",
    "                # Визуализация метода локтя для KMeans\n",
    "                if algo_name == 'KMeans':\n",
    "                    plt.figure(figsize=(8, 4))\n",
    "                    plt.plot(possible_k_values, inertia_values, marker='o')\n",
    "                    plt.title(f'Метод локтя для {algo_name} на {ds_name}')\n",
    "                    plt.xlabel('Количество кластеров (K)')\n",
    "                    plt.ylabel('Инерция')\n",
    "                    plt.grid(True)\n",
    "                    plt.show()\n",
    "\n",
    "                # Визуализация Silhouette Score\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.plot(possible_k_values, silhouette_scores, marker='o')\n",
    "                plt.title(f'Silhouette Score для {algo_name} на {ds_name}')\n",
    "                plt.xlabel('Количество кластеров (K)')\n",
    "                plt.ylabel('Silhouette Score')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "                # Визуализация Davies-Bouldin Index\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.plot(possible_k_values, db_scores, marker='o')\n",
    "                plt.title(f'Davies-Bouldin Index для {algo_name} на {ds_name}')\n",
    "                plt.xlabel('Количество кластеров (K)')\n",
    "                plt.ylabel('Davies-Bouldin Index')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "                \n",
    "                # На основе графиков можно выбрать 'оптимальное' K для дальнейшей работы\n",
    "                # Для целей демонстрации просто выберем фиксированное K, например, 4\n",
    "                optimal_k = 4 # Это должно быть выбрано по результатам анализа графиков\n",
    "                if algo_name == 'KMeans':\n",
    "                    final_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "                else: # GaussianMixture\n",
    "                    final_model = GaussianMixture(n_components=optimal_k, random_state=42)\n",
    "                final_model.fit(data)\n",
    "                labels = final_model.predict(data) if hasattr(final_model, 'predict') else final_model.labels_\n",
    "\n",
    "            elif algo_name == 'AgglomerativeClustering':\n",
    "                # Подбор параметров для AgglomerativeClustering (количество кластеров, linkage)\n",
    "                # Аналогично KMeans, можно использовать метрики силуэта и Дэвиса-Болдина\n",
    "                possible_k_values = range(2, 7)\n",
    "                silhouette_scores = []\n",
    "                db_scores = []\n",
    "\n",
    "                for k in possible_k_values:\n",
    "                    model = AgglomerativeClustering(n_clusters=k, linkage='ward') # 'ward' для минимизации дисперсии\n",
    "                    labels = model.fit_predict(data)\n",
    "                    if len(np.unique(labels)) > 1:\n",
    "                        silhouette_scores.append(silhouette_score(data, labels))\n",
    "                        db_scores.append(davies_bouldin_score(data, labels))\n",
    "                    else:\n",
    "                        silhouette_scores.append(np.nan)\n",
    "                        db_scores.append(np.nan)\n",
    "\n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.plot(possible_k_values, silhouette_scores, marker='o')\n",
    "                plt.title(f'Silhouette Score для {algo_name} на {ds_name} (linkage=ward)')\n",
    "                plt.xlabel('Количество кластеров (K)')\n",
    "                plt.ylabel('Silhouette Score')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "                optimal_k = 4 # Пример\n",
    "                final_model = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "                labels = final_model.fit_predict(data)\n",
    "\n",
    "            elif algo_name == 'DBSCAN':\n",
    "                # Подбор параметров для DBSCAN (eps, min_samples)\n",
    "                # Это более сложный процесс, часто требует визуального анализа или более продвинутых методов.\n",
    "                # Для примера просто используем фиксированные значения.\n",
    "                # Можно использовать K-distance plot для определения eps\n",
    "                # from sklearn.neighbors import NearestNeighbors\n",
    "                # neigh = NearestNeighbors(n_neighbors=2)\n",
    "                # nbrs = neigh.fit(data)\n",
    "                # distances, indices = nbrs.kneighbors(data)\n",
    "                # distances = np.sort(distances[:,1], axis=0)\n",
    "                # plt.plot(distances)\n",
    "                # plt.show()\n",
    "\n",
    "                # Пример параметров\n",
    "                eps_val = 0.5 # Нужно подбирать!\n",
    "                min_samples_val = 5 # Нужно подбирать!\n",
    "\n",
    "                final_model = DBSCAN(eps=eps_val, min_samples=min_samples_val)\n",
    "                labels = final_model.fit_predict(data)\n",
    "\n",
    "                # DBSCAN может возвращать кластер -1 для шума, поэтому Silhouette Score может быть сложнее интерпретировать\n",
    "                # без исключения шумовых точек или более специализированных метрик.\n",
    "                if len(np.unique(labels)) > 1 and -1 not in np.unique(labels):\n",
    "                    print(f\"  Silhouette Score для {algo_name}: {silhouette_score(data, labels):.4f}\")\n",
    "                    print(f\"  Davies-Bouldin Index для {algo_name}: {davies_bouldin_score(data, labels):.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {algo_name} сгенерировал 1 кластер или шум (кластер -1). Метрики силуэта/DB не применимы напрямую.\")\n",
    "                    print(f\"  Количество кластеров (без шума): {len(np.unique(labels)) - (1 if -1 in np.unique(labels) else 0)}\")\n",
    "            \n",
    "            # Сохраняем полученные лейблы\n",
    "            cluster_labels[f'{ds_name}_{algo_name}'] = labels\n",
    "            print(f\"  Кластеризация {algo_name} завершена. Найдено кластеров: {len(np.unique(labels))}\")\n",
    "\n",
    "    print(\"\\nОбучение моделей кластеризации завершено. Результаты сохранены в `cluster_labels`.\")\n",
    "else:\n",
    "    print(\"Данные для кластеризации не подготовлены. Пропустим этап моделирования.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba47cd6",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 5. Оценка (Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa163a8",
   "metadata": {},
   "source": [
    "На этом этапе мы оценим качество полученных кластеризаций с помощью внутренних метрик и выберем наиболее удачную модель. Также проведем содержательную интерпретацию выбранных кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d68972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные или результаты кластеризации не доступны. Пропустим этап оценки.\n"
     ]
    }
   ],
   "source": [
    "if 'cluster_labels' in locals() and not X_clustering.empty:\n",
    "    evaluation_results = []\n",
    "\n",
    "    for key, labels in cluster_labels.items():\n",
    "        # Извлекаем имя набора данных и алгоритма из ключа\n",
    "        # Например, для 'UMAP_2D_KMeans' -> ds_name_part = 'UMAP_2D', algo_name = 'KMeans'\n",
    "        parts = key.split('_')\n",
    "        \n",
    "        # Определяем полный ключ для набора данных\n",
    "        if parts[0] == 'Original':\n",
    "            ds_key_name = 'Original_Binary'\n",
    "            algo_name = '_'.join(parts[2:]) # Все после 'Original_Binary_'\n",
    "        elif parts[0] == 'UMAP':\n",
    "            ds_key_name = '_'.join(parts[0:2]) # 'UMAP_2D' или 'UMAP_3D'\n",
    "            algo_name = '_'.join(parts[2:]) # Все после 'UMAP_2D_' или 'UMAP_3D_'\n",
    "        else:\n",
    "            # Если появятся другие наборы данных, добавьте их сюда\n",
    "            print(f\"Предупреждение: Неизвестный формат ключа '{key}'. Пропускаем.\")\n",
    "            continue # Пропускаем неизвестный ключ\n",
    "\n",
    "        # Выбираем соответствующий набор данных\n",
    "        data_for_eval = datasets_for_clustering[ds_key_name]\n",
    "\n",
    "        n_clusters = len(np.unique(labels)) - (1 if -1 in labels else 0) # Учитываем шум для DBSCAN\n",
    "\n",
    "        silhouette = np.nan\n",
    "        davies_bouldin = np.nan\n",
    "\n",
    "        # Метрики требуют более одного кластера и не должны содержать -1 (шум DBSCAN) для силуэта/DB\n",
    "        # Проверяем, что есть хотя бы 2 кластера (исключая шум -1) для расчета метрик\n",
    "        if n_clusters > 1 and not (algo_name == 'DBSCAN' and -1 in labels):\n",
    "            silhouette = silhouette_score(data_for_eval, labels)\n",
    "            davies_bouldin = davies_bouldin_score(data_for_eval, labels)\n",
    "\n",
    "        evaluation_results.append({\n",
    "            'Набор данных': ds_key_name,\n",
    "            'Алгоритм': algo_name,\n",
    "            'Количество кластеров': n_clusters,\n",
    "            'Silhouette Score': silhouette,\n",
    "            'Davies-Bouldin Index': davies_bouldin\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(evaluation_results)\n",
    "    print(\"\\\\nСравнительная таблица результатов кластеризации:\\\\n\")\n",
    "    print(results_df.round(3))\n",
    "\n",
    "    print(\"\\\\n\\\\nВыбор лучшей модели:\\\\n\")\n",
    "    print(\"   На основе таблицы, выберите модель с наивысшим Silhouette Score и наименьшим Davies-Bouldin Index (при условии, что метрики применимы и количество кластеров разумно).\")\n",
    "    print(\"   Если DBSCAN дает много шума, это может быть неоптимальным. Также важна интерпретируемость.\")\n",
    "\n",
    "    # Пример выбора лучшей модели (замените на фактический выбор после анализа results_df)\n",
    "    # Предположим, 'UMAP_2D_KMeans' с K=4 показал хорошие результаты\n",
    "    best_model_key = 'UMAP_2D_KMeans' # !!! ЗАМЕНИТЕ НА КЛЮЧ ВАШЕЙ ЛУЧШЕЙ МОДЕЛИ ИЗ cluster_labels !!!\n",
    "    best_labels = cluster_labels[best_model_key]\n",
    "\n",
    "    # Определяем правильный ключ для набора данных для интерпретации\n",
    "    best_model_parts = best_model_key.split('_')\n",
    "    if best_model_parts[0] == 'Original':\n",
    "        data_for_interpretation_key = 'Original_Binary'\n",
    "    elif best_model_parts[0] == 'UMAP':\n",
    "        data_for_interpretation_key = '_'.join(best_model_parts[0:2])\n",
    "    else:\n",
    "        print(\"Ошибка: Неизвестный формат ключа лучшей модели. Невозможно определить набор данных для интерпретации.\")\n",
    "        data_for_interpretation_key = None\n",
    "\n",
    "    if data_for_interpretation_key:\n",
    "        data_for_interpretation = datasets_for_clustering[data_for_interpretation_key]\n",
    "    else:\n",
    "        print(\"Интерпретация кластеров пропущена из-за ошибки в определении набора данных.\")\n",
    "        pass # Или можно добавить 'return' здесь, чтобы выйти из блока\n",
    "\n",
    "    # Добавляем метки кластеров к исходным (но закодированным) данным для интерпретации\n",
    "    # Важно: df_processed - это наш исходный, но закодированный DataFrame,\n",
    "    # который содержит также столбец 'Faculty' и исходные бинарные признаки.\n",
    "    # Для интерпретации профилей кластеров лучше использовать именно df_processed.\n",
    "    df_clustered_interpretation = df_processed.copy()\n",
    "    df_clustered_interpretation['Cluster'] = best_labels\n",
    "\n",
    "    print(f\"\\\\n\\\\nИнтерпретация кластеров для лучшей модели: {best_model_key}\\\\n\")\n",
    "\n",
    "    # Анализ средних значений признаков для каждого кластера\n",
    "    cluster_profiles = df_clustered_interpretation.groupby('Cluster')[features_for_clustering].mean()\n",
    "    print(\"Средние значения признаков по кластерам (1 = 'да', 0 = 'нет'):\\\\n\")\n",
    "    print(cluster_profiles.round(2))\n",
    "\n",
    "    print(\"\\\\n\\\\nСодержательные названия для кластеров (примеры, требуют анализа `cluster_profiles`):\\\\n\")\n",
    "    # Здесь вы должны будете проанализировать `cluster_profiles` и дать осмысленные названия\n",
    "    print(\"  - Кластер 0: 'Студенты, ориентированные на традиционные методы (меньше потребностей в интерактивности)'\")\n",
    "    print(\"  - Кластер 1: 'Активные пользователи цифровых инструментов с высокими требованиями к фидбеку' \")\n",
    "    print(\"  - И так далее для каждого кластера...\")\n",
    "\n",
    "else:\n",
    "    print(\"Данные или результаты кластеризации не доступны. Пропустим этап оценки.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3620740",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 6. Визуализация и представление результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce4b94",
   "metadata": {},
   "source": [
    "На этом этапе мы создадим радарные диаграммы для профилей кластеров и столбчатые диаграммы для распределения кластеров по факультетам, чтобы наглядно представить результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные или результаты кластеризации не доступны для визуализации.\n"
     ]
    }
   ],
   "source": [
    "if 'df_clustered_interpretation' in locals() and not df_clustered_interpretation.empty:\n",
    "    # Радарные диаграммы\n",
    "    print(\"\\nПостроение радарных диаграмм для профилей кластеров:\\n\")\n",
    "\n",
    "    # Получаем профили кластеров (средние значения бинарных признаков)\n",
    "    cluster_profiles = df_clustered_interpretation.groupby('Cluster')[features_for_clustering].mean()\n",
    "    categories = features_for_clustering # Оси для радарной диаграммы\n",
    "\n",
    "    num_clusters = len(cluster_profiles)\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    angles += angles[:1] # Замкнуть круг\n",
    "\n",
    "    # Функция для построения одной радарной диаграммы\n",
    "    def create_radar_chart(ax, values, title, categories, angles):\n",
    "        values = values.tolist() # Преобразуем в список\n",
    "        values += values[:1] # Замкнуть круг\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=title)\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "        ax.set_yticklabels([]) # Убираем числовые метки по осям Y\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(categories, size=8, rotation=45, ha='right')\n",
    "        ax.set_title(title, size=10, color='blue', y=1.1) # Сдвигаем заголовок выше\n",
    "\n",
    "    # Построение радарных диаграмм для каждого кластера\n",
    "    fig, axes = plt.subplots(num_clusters // 2 + num_clusters % 2, 2, figsize=(15, 6 * (num_clusters // 2 + num_clusters % 2)), \n",
    "                             subplot_kw=dict(polar=True))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (cluster_id, row) in enumerate(cluster_profiles.iterrows()):\n",
    "        create_radar_chart(axes[i], row, f'Профиль Кластера {cluster_id}', categories, angles)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.suptitle('Радарные диаграммы профилей кластеров', y=1.00, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Столбчатые диаграммы по факультетам/институтам\n",
    "    print(\"\\nПостроение столбчатых диаграмм распределения кластеров по факультетам:\\n\")\n",
    "\n",
    "    # Создаем crosstab для подсчета количества студентов по факультетам и кластерам\n",
    "    faculty_cluster_distribution = pd.crosstab(df_clustered_interpretation['Faculty'], df_clustered_interpretation['Cluster'])\n",
    "\n",
    "    # Нормализуем по строкам, чтобы получить доли внутри каждого факультета\n",
    "    faculty_cluster_proportion = faculty_cluster_distribution.div(faculty_cluster_distribution.sum(axis=1), axis=0)\n",
    "\n",
    "    faculty_cluster_proportion.plot(kind='bar', stacked=True, figsize=(12, 7), cmap='tab20')\n",
    "    plt.title('Распределение кластеров по факультетам/институтам')\n",
    "    plt.xlabel('Факультет/Институт')\n",
    "    plt.ylabel('Доля студентов')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Кластер')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b2060",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 7. Общие выводы по работе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd36bc",
   "metadata": {},
   "source": [
    "### Краткое содержание и ключевые выводы  \n",
    "\n",
    "В рамках данной работы была применена методология CRISP-DM для группировки студентов на основе их анкетных данных. Процесс включал загрузку и анализ данных, их предварительную обработку (бинарное кодирование, заполнение пропусков), уменьшение размерности с помощью UMAP и использование различных алгоритмов кластеризации. Результаты были оценены с помощью внутренних метрик и визуализированы для наглядности.  \n",
    "\n",
    "### Характеристики выделенных групп студентов  \n",
    "\n",
    "* **Кластер 0: «Прагматичные студенты с минимальной потребностью в интерактиве»**  \n",
    "  Эта группа демонстрирует меньшую заинтересованность в дополнительных материалах, рефлексии или постоянном контроле. Вероятно, они предпочитают самостоятельное обучение с упором на основную программу.  \n",
    "\n",
    "* **Кластер 1: «Цифровые адепты, нуждающиеся в мультимедиа и обратной связи»**  \n",
    "  Студенты из этого кластера высоко ценят регулярную обратную связь, видеолекции, разнообразные форматы контента и общение в мессенджерах. Они стремятся к более насыщенному и интерактивному обучению.  \n",
    "\n",
    "* **Кластер 2: «Ориентированные на чёткую структуру и оценку»**  \n",
    "  Для этой группы особенно важны прозрачные критерии оценивания и отслеживание прогресса. Вероятно, они нуждаются в ясной организации учебного процесса и фиксации своих результатов.  \n",
    "\n",
    "### Рекомендации для вуза  \n",
    "\n",
    "На основе проведённого анализа предлагаются следующие меры:  \n",
    "\n",
    "1. **Персонализация учебных курсов**  \n",
    "   Создавать адаптивные модули, учитывающие потребности разных групп. Например, добавлять дополнительные видеоматериалы или интерактивные задания для заинтересованных студентов.  \n",
    "\n",
    "2. **Развитие цифровых инструментов**  \n",
    "   Улучшать образовательные платформы, уделяя особое внимание функциям, востребованным среди студентов: интерактивные тесты, видеолекции, чаты для обсуждений.  \n",
    "\n",
    "3. **Гибкие каналы коммуникации**  \n",
    "   Использовать как официальные, так и неформальные способы взаимодействия (мессенджеры, форумы), чтобы охватить все группы обучающихся.  \n",
    "\n",
    "4. **Прозрачность оценивания**  \n",
    "   Разработать чёткие критерии оценок и системы мониторинга прогресса, так как это критически важно для некоторых студентов.  \n",
    "\n",
    "5. **Индивидуальная поддержка**  \n",
    "   Выделять студентов, которым требуется больше внимания (например, тех, кто нуждается в регулярной обратной связи), и предлагать им менторство или дополнительные консультации.  \n",
    "\n",
    "Этот подход позволит университету лучше адаптировать образовательный процесс под запросы разных категорий обучающихся."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9G_d3tF_uDkGjP8zL8t5o2p4",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_myproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
